{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7251,"sourceType":"datasetVersion","datasetId":2798},{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":1732825,"sourceType":"datasetVersion","datasetId":1028436}],"dockerImageVersionId":30068,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport keras\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom tensorflow.keras.applications import VGG16, InceptionResNetV2\nfrom keras import regularizers\nfrom tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-29T08:48:13.000729Z","iopub.execute_input":"2024-03-29T08:48:13.001072Z","iopub.status.idle":"2024-03-29T08:48:13.581079Z","shell.execute_reply.started":"2024-03-29T08:48:13.001036Z","shell.execute_reply":"2024-03-29T08:48:13.580098Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dir = \"../input/emotion-detection-fer/train\" #passing the path with training images\ntest_dir = \"../input/emotion-detection-fer/test\"   #passing the path with testing images","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:48:16.277697Z","iopub.execute_input":"2024-03-29T08:48:16.278069Z","iopub.status.idle":"2024-03-29T08:48:16.281997Z","shell.execute_reply.started":"2024-03-29T08:48:16.278027Z","shell.execute_reply":"2024-03-29T08:48:16.281049Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"img_size = 48 #original size of the image","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:48:19.589584Z","iopub.execute_input":"2024-03-29T08:48:19.589935Z","iopub.status.idle":"2024-03-29T08:48:19.593745Z","shell.execute_reply.started":"2024-03-29T08:48:19.589907Z","shell.execute_reply":"2024-03-29T08:48:19.592724Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nData Augmentation\n--------------------------\nrotation_range = rotates the image with the amount of degrees we provide\nwidth_shift_range = shifts the image randomly to the right or left along the width of the image\nheight_shift range = shifts image randomly to up or below along the height of the image\nhorizontal_flip = flips the image horizontally\nrescale = to scale down the pizel values in our image between 0 and 1\nzoom_range = applies random zoom to our object\nvalidation_split = reserves some images to be used for validation purpose\n\"\"\"\n\ntrain_datagen = ImageDataGenerator(#rotation_range = 180,\n                                         width_shift_range = 0.1,\n                                         height_shift_range = 0.1,\n                                         horizontal_flip = True,\n                                         rescale = 1./255,\n                                         #zoom_range = 0.2,\n                                         validation_split = 0.2\n                                        )\nvalidation_datagen = ImageDataGenerator(rescale = 1./255,\n                                         validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:48:28.959526Z","iopub.execute_input":"2024-03-29T08:48:28.959884Z","iopub.status.idle":"2024-03-29T08:48:28.965725Z","shell.execute_reply.started":"2024-03-29T08:48:28.959853Z","shell.execute_reply":"2024-03-29T08:48:28.964710Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nApplying data augmentation to the images as we read \nthem from their respectivve directories\n\"\"\"\ntrain_generator = train_datagen.flow_from_directory(directory = train_dir,\n                                                    target_size = (img_size,img_size),\n                                                    batch_size = 64,\n                                                    color_mode = \"grayscale\",\n                                                    class_mode = \"categorical\",\n                                                    subset = \"training\"\n                                                   )\nvalidation_generator = validation_datagen.flow_from_directory( directory = test_dir,\n                                                              target_size = (img_size,img_size),\n                                                              batch_size = 64,\n                                                              color_mode = \"grayscale\",\n                                                              class_mode = \"categorical\",\n                                                              subset = \"validation\"\n                                                             )","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:48:31.798995Z","iopub.execute_input":"2024-03-29T08:48:31.799367Z","iopub.status.idle":"2024-03-29T08:49:13.371277Z","shell.execute_reply.started":"2024-03-29T08:48:31.799335Z","shell.execute_reply":"2024-03-29T08:49:13.370250Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Found 22968 images belonging to 7 classes.\nFound 1432 images belonging to 7 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nModeling\n\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 64,kernel_size = (3,3),padding = 'same',activation = 'relu',input_shape=(img_size,img_size,1)))\nmodel.add(MaxPool2D(pool_size = 2,strides = 2))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = 2,strides = 2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = 2,strides = 2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 256,kernel_size = (3,3),padding = 'same',activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = 2,strides = 2))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\nmodel.add(Dense(units = 128,activation = 'relu',kernel_initializer='he_normal'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units = 64,activation = 'relu',kernel_initializer='he_normal'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units = 32,activation = 'relu',kernel_initializer='he_normal'))\nmodel.add(Dense(7,activation = 'softmax'))\n\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48, 1)))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Conv2D(128, (5, 5), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(7, activation='softmax'))\n\nmodel.compile(\n    optimizer=Adam(lr=0.0001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:49:30.093997Z","iopub.execute_input":"2024-03-29T08:49:30.094371Z","iopub.status.idle":"2024-03-29T08:49:30.384572Z","shell.execute_reply.started":"2024-03-29T08:49:30.094340Z","shell.execute_reply":"2024-03-29T08:49:30.383889Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"epochs = 60\nbatch_size = 64","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:49:34.661947Z","iopub.execute_input":"2024-03-29T08:49:34.662324Z","iopub.status.idle":"2024-03-29T08:49:34.666625Z","shell.execute_reply.started":"2024-03-29T08:49:34.662290Z","shell.execute_reply":"2024-03-29T08:49:34.665711Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:49:37.327478Z","iopub.execute_input":"2024-03-29T08:49:37.327867Z","iopub.status.idle":"2024-03-29T08:49:37.342260Z","shell.execute_reply.started":"2024-03-29T08:49:37.327831Z","shell.execute_reply":"2024-03-29T08:49:37.341402Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_23 (Conv2D)           (None, 48, 48, 32)        320       \n_________________________________________________________________\nconv2d_24 (Conv2D)           (None, 48, 48, 64)        18496     \n_________________________________________________________________\nbatch_normalization_30 (Batc (None, 48, 48, 64)        256       \n_________________________________________________________________\nmax_pooling2d_20 (MaxPooling (None, 24, 24, 64)        0         \n_________________________________________________________________\nconv2d_25 (Conv2D)           (None, 24, 24, 128)       204928    \n_________________________________________________________________\nbatch_normalization_31 (Batc (None, 24, 24, 128)       512       \n_________________________________________________________________\nmax_pooling2d_21 (MaxPooling (None, 12, 12, 128)       0         \n_________________________________________________________________\nconv2d_26 (Conv2D)           (None, 12, 12, 512)       590336    \n_________________________________________________________________\nbatch_normalization_32 (Batc (None, 12, 12, 512)       2048      \n_________________________________________________________________\nmax_pooling2d_22 (MaxPooling (None, 6, 6, 512)         0         \n_________________________________________________________________\nconv2d_27 (Conv2D)           (None, 6, 6, 512)         2359808   \n_________________________________________________________________\nbatch_normalization_33 (Batc (None, 6, 6, 512)         2048      \n_________________________________________________________________\nmax_pooling2d_23 (MaxPooling (None, 3, 3, 512)         0         \n_________________________________________________________________\nconv2d_28 (Conv2D)           (None, 3, 3, 512)         2359808   \n_________________________________________________________________\nbatch_normalization_34 (Batc (None, 3, 3, 512)         2048      \n_________________________________________________________________\nmax_pooling2d_24 (MaxPooling (None, 2, 2, 512)         0         \n_________________________________________________________________\nconv2d_29 (Conv2D)           (None, 2, 2, 512)         2359808   \n_________________________________________________________________\nbatch_normalization_35 (Batc (None, 2, 2, 512)         2048      \n_________________________________________________________________\nmax_pooling2d_25 (MaxPooling (None, 1, 1, 512)         0         \n_________________________________________________________________\nconv2d_30 (Conv2D)           (None, 1, 1, 512)         2359808   \n_________________________________________________________________\nbatch_normalization_36 (Batc (None, 1, 1, 512)         2048      \n_________________________________________________________________\nmax_pooling2d_26 (MaxPooling (None, 1, 1, 512)         0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 512)               0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 256)               131328    \n_________________________________________________________________\nbatch_normalization_37 (Batc (None, 256)               1024      \n_________________________________________________________________\ndense_13 (Dense)             (None, 512)               131584    \n_________________________________________________________________\nbatch_normalization_38 (Batc (None, 512)               2048      \n_________________________________________________________________\ndense_14 (Dense)             (None, 512)               262656    \n_________________________________________________________________\nbatch_normalization_39 (Batc (None, 512)               2048      \n_________________________________________________________________\ndense_15 (Dense)             (None, 512)               262656    \n_________________________________________________________________\nbatch_normalization_40 (Batc (None, 512)               2048      \n_________________________________________________________________\ndense_16 (Dense)             (None, 512)               262656    \n_________________________________________________________________\nbatch_normalization_41 (Batc (None, 512)               2048      \n_________________________________________________________________\ndense_17 (Dense)             (None, 7)                 3591      \n=================================================================\nTotal params: 11,328,007\nTrainable params: 11,317,895\nNon-trainable params: 10,112\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(x = train_generator,epochs = epochs,validation_data = validation_generator)","metadata":{"execution":{"iopub.status.busy":"2024-03-29T08:49:41.703622Z","iopub.execute_input":"2024-03-29T08:49:41.703956Z","iopub.status.idle":"2024-03-29T09:35:15.015442Z","shell.execute_reply.started":"2024-03-29T08:49:41.703928Z","shell.execute_reply":"2024-03-29T09:35:15.014583Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/60\n359/359 [==============================] - 217s 592ms/step - loss: 22.2145 - accuracy: 0.1840 - val_loss: 19.3073 - val_accuracy: 0.1704\nEpoch 2/60\n359/359 [==============================] - 41s 114ms/step - loss: 18.5020 - accuracy: 0.2591 - val_loss: 17.1580 - val_accuracy: 0.2709\nEpoch 3/60\n359/359 [==============================] - 41s 114ms/step - loss: 16.6211 - accuracy: 0.2970 - val_loss: 15.1400 - val_accuracy: 0.3184\nEpoch 4/60\n359/359 [==============================] - 41s 115ms/step - loss: 14.5323 - accuracy: 0.3511 - val_loss: 12.8993 - val_accuracy: 0.3973\nEpoch 5/60\n359/359 [==============================] - 41s 113ms/step - loss: 12.4115 - accuracy: 0.3929 - val_loss: 10.8988 - val_accuracy: 0.4204\nEpoch 6/60\n359/359 [==============================] - 41s 113ms/step - loss: 10.4170 - accuracy: 0.4186 - val_loss: 9.3226 - val_accuracy: 0.3792\nEpoch 7/60\n359/359 [==============================] - 46s 129ms/step - loss: 8.6044 - accuracy: 0.4428 - val_loss: 7.3566 - val_accuracy: 0.4818\nEpoch 8/60\n359/359 [==============================] - 41s 114ms/step - loss: 6.9911 - accuracy: 0.4750 - val_loss: 5.9709 - val_accuracy: 0.4909\nEpoch 9/60\n359/359 [==============================] - 51s 141ms/step - loss: 5.6436 - accuracy: 0.5002 - val_loss: 4.9596 - val_accuracy: 0.4358\nEpoch 10/60\n359/359 [==============================] - 52s 144ms/step - loss: 4.5614 - accuracy: 0.5126 - val_loss: 3.9217 - val_accuracy: 0.5175\nEpoch 11/60\n359/359 [==============================] - 42s 116ms/step - loss: 3.7081 - accuracy: 0.5330 - val_loss: 3.3517 - val_accuracy: 0.4832\nEpoch 12/60\n359/359 [==============================] - 41s 113ms/step - loss: 3.0684 - accuracy: 0.5381 - val_loss: 2.7465 - val_accuracy: 0.5447\nEpoch 13/60\n359/359 [==============================] - 41s 113ms/step - loss: 2.5864 - accuracy: 0.5567 - val_loss: 2.3820 - val_accuracy: 0.5182\nEpoch 14/60\n359/359 [==============================] - 42s 116ms/step - loss: 2.2319 - accuracy: 0.5572 - val_loss: 2.1101 - val_accuracy: 0.5440\nEpoch 15/60\n359/359 [==============================] - 42s 116ms/step - loss: 1.9738 - accuracy: 0.5745 - val_loss: 1.8630 - val_accuracy: 0.5712\nEpoch 16/60\n359/359 [==============================] - 40s 113ms/step - loss: 1.7969 - accuracy: 0.5796 - val_loss: 1.8096 - val_accuracy: 0.5503\nEpoch 17/60\n359/359 [==============================] - 42s 116ms/step - loss: 1.6681 - accuracy: 0.5880 - val_loss: 1.6935 - val_accuracy: 0.5531\nEpoch 18/60\n359/359 [==============================] - 43s 121ms/step - loss: 1.5798 - accuracy: 0.5898 - val_loss: 1.6170 - val_accuracy: 0.5545\nEpoch 19/60\n359/359 [==============================] - 43s 119ms/step - loss: 1.5132 - accuracy: 0.6042 - val_loss: 1.6186 - val_accuracy: 0.5552\nEpoch 20/60\n359/359 [==============================] - 42s 117ms/step - loss: 1.4493 - accuracy: 0.6112 - val_loss: 1.5627 - val_accuracy: 0.5628\nEpoch 21/60\n359/359 [==============================] - 43s 120ms/step - loss: 1.4077 - accuracy: 0.6162 - val_loss: 1.5836 - val_accuracy: 0.5635\nEpoch 22/60\n359/359 [==============================] - 43s 119ms/step - loss: 1.3854 - accuracy: 0.6243 - val_loss: 1.5611 - val_accuracy: 0.5349\nEpoch 23/60\n359/359 [==============================] - 43s 119ms/step - loss: 1.3648 - accuracy: 0.6218 - val_loss: 1.4878 - val_accuracy: 0.5670\nEpoch 24/60\n359/359 [==============================] - 46s 128ms/step - loss: 1.3248 - accuracy: 0.6293 - val_loss: 1.6506 - val_accuracy: 0.5182\nEpoch 25/60\n359/359 [==============================] - 44s 124ms/step - loss: 1.3050 - accuracy: 0.6417 - val_loss: 1.4909 - val_accuracy: 0.5684\nEpoch 26/60\n359/359 [==============================] - 41s 115ms/step - loss: 1.2843 - accuracy: 0.6469 - val_loss: 1.5669 - val_accuracy: 0.5384\nEpoch 27/60\n359/359 [==============================] - 43s 119ms/step - loss: 1.2713 - accuracy: 0.6498 - val_loss: 1.5270 - val_accuracy: 0.5370\nEpoch 28/60\n359/359 [==============================] - 42s 118ms/step - loss: 1.2392 - accuracy: 0.6671 - val_loss: 1.3785 - val_accuracy: 0.6075\nEpoch 29/60\n359/359 [==============================] - 42s 118ms/step - loss: 1.2293 - accuracy: 0.6694 - val_loss: 1.4123 - val_accuracy: 0.5999\nEpoch 30/60\n359/359 [==============================] - 41s 115ms/step - loss: 1.2382 - accuracy: 0.6641 - val_loss: 1.3873 - val_accuracy: 0.5908\nEpoch 31/60\n359/359 [==============================] - 43s 121ms/step - loss: 1.2085 - accuracy: 0.6781 - val_loss: 1.6487 - val_accuracy: 0.5342\nEpoch 32/60\n359/359 [==============================] - 42s 117ms/step - loss: 1.2053 - accuracy: 0.6787 - val_loss: 1.4873 - val_accuracy: 0.5705\nEpoch 33/60\n359/359 [==============================] - 41s 114ms/step - loss: 1.2085 - accuracy: 0.6797 - val_loss: 1.4819 - val_accuracy: 0.5803\nEpoch 34/60\n359/359 [==============================] - 41s 115ms/step - loss: 1.1764 - accuracy: 0.6897 - val_loss: 1.4554 - val_accuracy: 0.5957\nEpoch 35/60\n359/359 [==============================] - 41s 115ms/step - loss: 1.1899 - accuracy: 0.6838 - val_loss: 1.6528 - val_accuracy: 0.5272\nEpoch 36/60\n359/359 [==============================] - 41s 113ms/step - loss: 1.1819 - accuracy: 0.6916 - val_loss: 1.8527 - val_accuracy: 0.4469\nEpoch 37/60\n359/359 [==============================] - 44s 123ms/step - loss: 1.1609 - accuracy: 0.6979 - val_loss: 1.5728 - val_accuracy: 0.5817\nEpoch 38/60\n359/359 [==============================] - 41s 114ms/step - loss: 1.1507 - accuracy: 0.7112 - val_loss: 1.8522 - val_accuracy: 0.5349\nEpoch 39/60\n359/359 [==============================] - 41s 114ms/step - loss: 1.1534 - accuracy: 0.7055 - val_loss: 1.4695 - val_accuracy: 0.5838\nEpoch 40/60\n359/359 [==============================] - 41s 115ms/step - loss: 1.1466 - accuracy: 0.7120 - val_loss: 1.6276 - val_accuracy: 0.5677\nEpoch 41/60\n359/359 [==============================] - 42s 116ms/step - loss: 1.1591 - accuracy: 0.7126 - val_loss: 1.5792 - val_accuracy: 0.5740\nEpoch 42/60\n359/359 [==============================] - 41s 114ms/step - loss: 1.1311 - accuracy: 0.7171 - val_loss: 1.8036 - val_accuracy: 0.4679\nEpoch 43/60\n359/359 [==============================] - 41s 114ms/step - loss: 1.1358 - accuracy: 0.7154 - val_loss: 1.4521 - val_accuracy: 0.5873\nEpoch 44/60\n359/359 [==============================] - 41s 115ms/step - loss: 1.1124 - accuracy: 0.7250 - val_loss: 1.4633 - val_accuracy: 0.5957\nEpoch 45/60\n359/359 [==============================] - 44s 122ms/step - loss: 1.1161 - accuracy: 0.7264 - val_loss: 1.6984 - val_accuracy: 0.5873\nEpoch 46/60\n359/359 [==============================] - 41s 114ms/step - loss: 1.0912 - accuracy: 0.7401 - val_loss: 1.7685 - val_accuracy: 0.5370\nEpoch 47/60\n359/359 [==============================] - 43s 120ms/step - loss: 1.1092 - accuracy: 0.7328 - val_loss: 1.6181 - val_accuracy: 0.5559\nEpoch 48/60\n359/359 [==============================] - 46s 129ms/step - loss: 1.1110 - accuracy: 0.7308 - val_loss: 1.5529 - val_accuracy: 0.5810\nEpoch 49/60\n359/359 [==============================] - 45s 124ms/step - loss: 1.0936 - accuracy: 0.7433 - val_loss: 1.9097 - val_accuracy: 0.4944\nEpoch 50/60\n359/359 [==============================] - 42s 118ms/step - loss: 1.1002 - accuracy: 0.7385 - val_loss: 1.5179 - val_accuracy: 0.5796\nEpoch 51/60\n359/359 [==============================] - 42s 118ms/step - loss: 1.0860 - accuracy: 0.7448 - val_loss: 1.5995 - val_accuracy: 0.5649\nEpoch 52/60\n359/359 [==============================] - 46s 129ms/step - loss: 1.0708 - accuracy: 0.7552 - val_loss: 1.4584 - val_accuracy: 0.6313\nEpoch 53/60\n359/359 [==============================] - 48s 133ms/step - loss: 1.0907 - accuracy: 0.7417 - val_loss: 1.8148 - val_accuracy: 0.5021\nEpoch 54/60\n359/359 [==============================] - 44s 123ms/step - loss: 1.0687 - accuracy: 0.7559 - val_loss: 1.9649 - val_accuracy: 0.4378\nEpoch 55/60\n359/359 [==============================] - 43s 120ms/step - loss: 1.0745 - accuracy: 0.7544 - val_loss: 1.6505 - val_accuracy: 0.5887\nEpoch 56/60\n359/359 [==============================] - 43s 119ms/step - loss: 1.0561 - accuracy: 0.7585 - val_loss: 1.6962 - val_accuracy: 0.5615\nEpoch 57/60\n359/359 [==============================] - 41s 114ms/step - loss: 1.0587 - accuracy: 0.7607 - val_loss: 1.6604 - val_accuracy: 0.5726\nEpoch 58/60\n359/359 [==============================] - 41s 115ms/step - loss: 1.0458 - accuracy: 0.7665 - val_loss: 1.7202 - val_accuracy: 0.5196\nEpoch 59/60\n359/359 [==============================] - 42s 116ms/step - loss: 1.0414 - accuracy: 0.7742 - val_loss: 1.6423 - val_accuracy: 0.5971\nEpoch 60/60\n359/359 [==============================] - 42s 116ms/step - loss: 1.0500 - accuracy: 0.7691 - val_loss: 1.9353 - val_accuracy: 0.4721\n","output_type":"stream"}]},{"cell_type":"code","source":"fig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nfig.set_size_inches(12,4)\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Training Accuracy vs Validation Accuracy')\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epoch')\nax[0].legend(['Train', 'Validation'], loc='upper left')\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Training Loss vs Validation Loss')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epoch')\nax[1].legend(['Train', 'Validation'], loc='upper left')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model_optimal.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = image.load_img(\"../input/emotion-detection-fer/test/happy/im1021.png\",target_size = (48,48),color_mode = \"grayscale\")\nimg = np.array(img)\nplt.imshow(img)\nprint(img.shape) #prints (48,48) that is the shape of our image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Neutral',5:'Sad',6:'Surprise'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = np.expand_dims(img,axis = 0) #makes image shape (1,48,48)\nimg = img.reshape(1,48,48,1)\nresult = model.predict(img)\nresult = list(result[0])\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_index = result.index(max(result))\nprint(label_dict[img_index])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss, train_acc = model.evaluate(train_generator)\ntest_loss, test_acc   = model.evaluate(validation_generator)\nprint(\"final train accuracy = {:.2f} , validation accuracy = {:.2f}\".format(train_acc*100, test_acc*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('model_weights.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}